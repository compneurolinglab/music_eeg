## Modules

This directory contains modular components used in the **Music-EEG Cognition** project. Each submodule performs a specific stage in the overall processing pipeline, including data preprocessing, feature extraction, transfer learning, and demo inference.

### Submodules

#### `Demo/`
Includes interactive notebooks demonstrating how to process audio data, extract features, synthesize speech, and perform CNN-based predictions.

- `feature_extraction.ipynb`: Extracts acoustic features from audio stimuli.
- `merge_features.ipynb`: Merges feature sets for model input.
- `textToSpeech.ipynb`: Converts text into synthetic speech using TTS.
- `demo_cnn_prediction.ipynb`: Shows prediction results from the trained CNN model.

> **Note**: Audio files in the `Stimuli` folder are in MP3 format. Please convert them to 16kHz WAV format for compatibility with feature extraction and model input.

#### `cnn_transfer/`
Contains core scripts for training the CNN using transfer learning and preprocessing audio into cochleagram representations.

- `cochleagram_generator.py`: Generates cochleagram images from audio signals.
- `TransferLearningCNN.py`: Defines and fine-tunes the CNN model using transfer learning techniques.

> The cochleagrams generated by `cochleagram_generator.py` are used as input to the CNN defined in `TransferLearningCNN.py`.

---

Each module is designed to be modular and reusable across different stages of the pipeline. For further details, refer to the README files in each subfolder.
